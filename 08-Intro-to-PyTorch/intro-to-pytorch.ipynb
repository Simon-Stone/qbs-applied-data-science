{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style='float:right;max-width:30%;'>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/PyTorch_logo_black.svg/640px-PyTorch_logo_black.svg.png' style='padding:10px;background-color:white'>\n",
    "<figcaption style='text-align:right'>Source: <a href=https://commons.wikimedia.org/wiki/File:PyTorch_logo_black.svg>Wikimedia Commons</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "PyTorch is a machine learning framework with a major focus on neural networks used for computer vision, audio and natural language processing. The user-facing frontend is written in Python, but the number-crunching is handled by a more optimized C++ backend, including support for outsourcing computations to graphics cards (GPUs) for a substantial increase in speed. PyTorch was originally created by Meta (formerly known as facebook), but has always been open source, permissively licensed ([BSD-3](https://en.wikipedia.org/wiki/BSD_licenses#3-clause)), and since September 2022 is managed by the non-profit PyTorch Foundation, a subsidiary of the [Linux Foundation](https://en.wikipedia.org/wiki/Linux_Foundation).\n",
    "\n",
    "The accessible interface, huge community, and optimized implementations have established PyTorch among the top choices for education, research, and production in the field of neural network design.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>Caveat emptor:</b> PyTorch is not really \"better\" or \"worse\" than other popular frameworks like [Keras](https://keras.io/) or [TensorFlow](https://www.tensorflow.org/). While each framework has its particular strengths, they differ more in their style, philosophy, and user base than in their feature lists and performance. You should absolutely explore other options available to you and find what you like best!\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is directed at an audience with a fundamental understanding of neural networks. If you are completely new to this field, you might want to check out the following articles:\n",
    "\n",
    "- [A Gentle Introduction to Neural Networks](https://towardsdatascience.com/a-gentle-introduction-to-neural-networks-series-part-1-2b90b87795bc)\n",
    "- [Build Neural Network from Scratch](https://towardsdatascience.com/build-neural-network-from-scratch-part-2-673ec7cdd89f)\n",
    "- [Machine Learning for Beginners: An Introduction to Neural Networks](https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9)\n",
    "\n",
    "A good command of the Python programming language is also required to fully understand the code presented in this notebook. If you have no experience with Python, there are innumerable great tutorials and introductions out there. You can find a good overview of some of them [here](https://wiki.python.org/moin/BeginnersGuide/Programmers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core package of PyTorch is called [`torch`](https://pypi.org/project/torch/). This package contains all the code required to setup and compute general purpose neural networks. It is extended by packages that offer more specialized functions and objects specific to various applications: [`torchvision`](https://pytorch.org/vision/stable/index.html) for Computer Vision (working with images or videos), [`torchaudio`](https://pytorch.org/audio/stable/index.html) for audio processing (e.g. speech recognition or synthesis), and [`torchtext`](https://pytorch.org/text/stable/index.html) for natural language processing. PyTorch is extended by various other packages that comprise the [*PyTorch Ecosystem*](https://pytorch.org/ecosystem/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plethora of functions and objects can be found within PyTorch. But arguably the most important basic components are:\n",
    "\n",
    "1. The [`Tensor`](https://pytorch.org/docs/stable/tensors.html#tensor-class-reference) class\n",
    "2. The differentiation engine [`Autograd`](https://pytorch.org/docs/stable/autograd.html#module-torch.autograd)\n",
    "3. The neural network building blocks (layers and activation functions) found in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#module-torch.nn)\n",
    "\n",
    "Before we build our first neural network from scratch, let us walk through these components one at a time:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The `Tensor` class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style='float:right;max-width=10%;'>\n",
    "<img src=https://imgs.xkcd.com/comics/machine_learning.png style='padding-right:10px'>\n",
    "<figcaption>Source: <a href=https://xkcd.com/license.html>XKCD</a> </figcaption>\n",
    "</figure>\n",
    "\n",
    "Neural networks are essentially a sequence of linear algebra operations. A [mathematical tensor](https://en.wikipedia.org/wiki/Tensor) is the most general algebraic object, of which simpler algebraic objects can be derived:\n",
    "\n",
    "- A scalar is a tensor of rank 0:\n",
    "$$\n",
    "\\left[ 0 \\right]\n",
    "$$\n",
    "- A vector is a tensor of rank 1 (a.k.a. a collection of rank 0 tensors):\n",
    "$$\n",
    "\\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix}\n",
    "$$\n",
    "- A matrix is a tensor of rank 2 (a.k.a. a collection of rank 1 tensors):\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 3 \\right], \\left[ 4 \\right], \\left[ 5 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 6 \\right], \\left[ 7 \\right], \\left[ 8 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- An $n$-dimensional array is a tensor of rank $n$ (a.k.a. a collection of rank $n-1$ tensors):\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 0 \\right], \\left[ 1 \\right], \\left[ 2 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 3 \\right], \\left[ 4 \\right], \\left[ 5 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 6 \\right], \\left[ 7 \\right], \\left[ 8 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}, \n",
    "\\begin{bmatrix} \n",
    "  \\begin{bmatrix} \\left[ 9 \\right], \\left[ 10 \\right], \\left[ 11 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 12 \\right], \\left[ 13 \\right], \\left[ 14 \\right] \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} \\left[ 15 \\right], \\left[ 16 \\right], \\left[ 17 \\right] \\end{bmatrix} \n",
    "\\end{bmatrix}, \n",
    "\\ldots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Describing a mathematical tensor as a generalized matrix is not [the whole story](https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c). For the purposes of this introduction, this simplified definition shall, however, suffice.\n",
    "\n",
    "In PyTorch, everything runs on tensors: Your data is encoded in a tensor, the neural networks are expressed as tensors, sending the data through the network is a series of transformations on a tensor. All of these tensors are represented by a class named [`Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor) found in the core `torch` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.Tensor([[0, 1, 2], [3, 4, 5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Creating a `Tensor`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [many ways to conveniently create tensors](https://pytorch.org/docs/stable/torch.html#creation-ops) from existing data, with specific initializations, or of specific shapes. Let's try out some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A rank 3 tensor filled with zeros: \\n\", zeros := torch.zeros(2, 2, 2))\n",
    "print(\"A tensor of the same shape but filled with ones: \\n\", torch.ones_like(zeros))\n",
    "print(\"A rank 2 tensor representing an identity matrix:\\n\", torch.eye(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Indexing, slicing, operations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the central data structure in `PyTorch`, `Tensor` objects support all features of a normal multi-dimensional array. You can access individual elements in a `Tensor` by using Python's or `numpy`'s regular indexing and slicing notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n",
    "print(t)\n",
    "print(\"Python-style indexing:\", t[1][2])\n",
    "print(\"Numpy-style indexing:\", t[0, 1])\n",
    "print(\"Python-style slicing (first row):\", t[0][:])\n",
    "print(\"Numpy-style slicing (last column):\", t[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform basic calculations with tensors just as we would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 1, 1])\n",
    "b = torch.tensor([2, 2, 2])\n",
    "\n",
    "print(f'{a = }, {b = }')\n",
    "print(f'Addition: {a + b = }')\n",
    "print(f'Element-wise product: {a * b = }')\n",
    "print(f'Element-wise division: {a / b = }')\n",
    "print(f\"Matrix multiplicaiton: {a @ b = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to basic operations, a number of methods and functions are provided to mirror the functionality offered by `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)\n",
    "print(\"Find the maximum value in the tensor:\", t.max())\n",
    "print(\"Find the minimum value in the tensor:\", t.min())\n",
    "print(\"Calculate the sum along the first dimension:\", t.sum(dim=0))\n",
    "print(\"Calculate the sine of each element: \", t.sin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes `Tensor`s special, however, is the added functionality specifically designed for machine learning workflows.\n",
    "\n",
    "We can, for example, move a tensor to the computer's GPU. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>Note:</b> If you tried to do anything GPU-related on a computer without GPU-access, your program would fail ungracefully. But you should always keep portability in mind: Machine-learning code is often developed and tested on a local machine (e.g. a laptop) and then moved to a cluster or other high-performance computer to do the actual number crunching. \n",
    "</div>\n",
    "\n",
    "There is, however, a way to make sure we only use a device that is actually available on the current machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  # CUDA is usually the most desirable backend\n",
    "    backend = 'cuda'\n",
    "elif torch.backends.mps.is_available():  # MPS is supported on some Apple Silicon systems\n",
    "    backend = 'mps'\n",
    "else:\n",
    "    backend = 'cpu'\n",
    "\n",
    "device = torch.device(backend)\n",
    "print(f'Using {backend.upper()} backend!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can safely move a tensor to a different device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.to(device)\n",
    "t.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all subsequent operations involving the `Tensor` are going to run on the GPU (if available)!\n",
    "\n",
    "Another major benefit of using `Tensor`s over regular `numpy` arrays is the automatic calculation of gradients, which we will learn more about in the next section. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The `Autograd` differentiation engine\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "This section is in large parts taken from the [`PyTorch` tutorial \"The Fundamentals of Autograd\"](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#what-do-we-need-autograd-for)!\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Gradients in neural network training\n",
    "Calculating gradients is *the* most important computation when training neural networks. If you would like a quick reminder why, read on. Otherwise, you can skip ahead to the [TL;DR](https://en.wikipedia.org/wiki/TL;DR) at the end of this section, or even straight to next section.\n",
    "\n",
    "A machine learning model is a function, with inputs and outputs. For this discussion, we’ll treat the inputs as an i-dimensional vector $\\vec{x}$, with elements $x_{i}$. We can then express the model, $M$, as a vector-valued function of the input: \n",
    "$$\n",
    "\\vec{y} = \\vec{M}\\left(\\vec{x}\\right) \n",
    "$$\n",
    "(We treat the value of $M$’s output as a vector because in general, a model may have any number of outputs.)\n",
    "\n",
    "Since we’ll mostly be discussing autograd in the context of training, our output of interest will be the model’s loss. The loss function \n",
    "$$\n",
    "L\\left(\\vec{y}\\right) = L\\left(\\vec{M}\\right)\n",
    "$$ \n",
    "\n",
    "is a single-valued scalar function of the model’s output. This function expresses how far off our model’s prediction was from a particular input’s ideal output. *Note:* After this point, we will often omit the vector sign where it should be contextually clear - e.g., $y$ instead of $\\vec{y}$.\n",
    "\n",
    "In training a model, we want to minimize the loss. In the idealized case of a perfect model, that means adjusting its learning weights - that is, the adjustable parameters of the function - such that loss is zero for all inputs. In the real world, it means an iterative process of nudging the learning weights until we see that we get a tolerable loss for a wide variety of inputs.\n",
    "\n",
    "How do we decide how far and in which direction to nudge the weights? We want to minimize the loss, which means making its first derivative with respect to the input equal to 0: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = 0 \n",
    "$$\n",
    "\n",
    "Recall, though, that the loss is not directly derived from the input, but a function of the model’s output (which is a function of the input directly):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x}  = \\frac{\\partial {L({\\vec y})}}{\\partial x} \n",
    "$$\n",
    "\n",
    "By the chain rule of differential calculus, we have \n",
    "\n",
    "$$\n",
    "\\frac{\\partial {L({\\vec y})}}{\\partial x} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}  = \\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}.$$\n",
    "\n",
    "$\\frac{\\partial M(x)}{\\partial x}$ is where things get complex. The partial derivatives of the model’s outputs with respect to its inputs, if we were to expand the expression using the chain rule again, would involve many local partial derivatives over every multiplied learning weight, every activation function, and every other mathematical transformation in the model. The full expression for each such partial derivative is the sum of the products of the local gradient of every possible path through the computation graph that ends with the variable whose gradient we are trying to measure.\n",
    "\n",
    "In particular, the gradients over the learning weights are of interest to us - they tell us what direction to change each weight to get the loss function closer to zero.\n",
    "\n",
    "Since the number of such local derivatives (each corresponding to a separate path through the model’s computation graph) will tend to go up exponentially with the depth of a neural network, so does the complexity in computing them. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "**TL;DR:**\n",
    "Training a neural network means adjusting its parameters so that its output is \"as correct as possible\". The gradient of the error tells us how a change of the parameters affects the correctness of the output and is therefore *the* essential driver of the training process. Calculating gradients with respect to every single parameter, however, is complex and resource-intensive.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 `Autograd` to the rescue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where `Autograd` comes in: It tracks the history of every computation performed on a tensor. Every computed tensor in your `PyTorch` model carries a history of its input tensors and the function used to create it. Combined with the fact that `PyTorch` functions meant to act on tensors each have a built-in implementation for computing their own derivatives, this greatly speeds up the computation of the local derivatives needed for learning.\n",
    "\n",
    "Let's look at a simple example: We will create a set of equidistant values between $0$ and $2\\pi$ and then apply a few functions to it. Afterwards we will walk *backwards* through the sequence of calculations and differentiate every step long the way.\n",
    "\n",
    "First up, we create a tensor of 25 linearly spaced values on the interval $[0, 2\\pi]$. By default, `Autograd` will not track the gradient of tensors created in this way. We have to set `requires_grad` explicitly to `True`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do a calculation on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sin(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result tensor `b` has a property called `grad_fn` that tells us that it is the result of a `sin` operation!\n",
    "\n",
    "Let's do some more computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2 * b\n",
    "print(c)\n",
    "d = c + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s compute a single-element output, as is the case when computing a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = d.sum()\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `grad_fn` stored with our tensors allows you to walk the computation all the way back to its inputs with its `next_functions` property. We can drill down on this property to show us the gradient functions for all the prior tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('out:')\n",
    "print(out.grad_fn)\n",
    "print(out.grad_fn.next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nd:')\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nc:')\n",
    "print(c.grad_fn)\n",
    "print('\\nb:')\n",
    "print(b.grad_fn)\n",
    "print('\\na:')\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `a.grad_fn` is reported as `None`, indicating that this was an input to the function with no history of its own.\n",
    "\n",
    "With all this machinery in place, how do we get derivatives out? You call the `backward()` method on the output, and check the input’s `grad` property to inspect the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualize this in a second, but let's try to figure out what we *should* see here. Recall that the computations we did were the following:\n",
    "\n",
    "$$\n",
    "d = 2 \\cdot \\sin\\left(a\\right) + 1\n",
    "$$\n",
    "\n",
    "So the derivative with respect to $a$ should be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial a} = 2\\cdot \\cos\\left(a\\right)\n",
    "$$\n",
    "\n",
    "Let's check this by visualizing our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We need to call the method detach() to signal that the gradients should not be tracked from this point on\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 Success! 👏\n",
    "\n",
    "We will see more of `Autograd` in action later, but first we need to talk about how to build neural networks in `PyTorch`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building blocks for neural networks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style='float:right;width:30%'>\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/densenet1.png\">\n",
    "<figcaption>\n",
    "\n",
    "The many layers of [DenseNet](https://pytorch.org/hub/pytorch_vision_densenet/)\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Neural networks are made up of a sequence of layers. Unsurprisingly, `PyTorch` offers a great variety of layers as building blocks to string together any desired architecture. They can be found alongside various activation functions in the submodule [`torch.nn`](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "You can find descriptions of each layer [in the documentation](https://pytorch.org/docs/stable/nn.html#module-torch.nn), but let's quickly mention some of the basic layers together:\n",
    "\n",
    "- [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): A linear transformation of the incoming data:\n",
    "  $$ y = xA^\\mathrm{T} + b$$\n",
    "- [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d): A 2D convolution over the incoming data\n",
    "- [`torch.nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm): Applies a long short-term memory RNN to the input data\n",
    "- [`torch.nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout): A dropout layer to randomly zero some of the input values during training\n",
    "\n",
    "The number of available layer types constantly increases as new architectures are developed in the field. In addition to these, `torch.nn` contains a number of containers to facilitate composing multiple layers into a neural network, which we will do in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A neural network from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design a neural network to classify flowers from the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)! \n",
    "\n",
    "We can read the data from the provided CSV file and split it into train and test set using [`pandas`](https://pandas.pydata.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data/iris.csv')\n",
    "# Encode species\n",
    "class_names = dataset['class'].unique()\n",
    "dataset['class'] = dataset['class'].map({name: idx for idx, name in enumerate(class_names)})\n",
    "\n",
    "# Split data randomly into training (90 %) and test (10 %) sets\n",
    "training_data = dataset.sample(frac=0.9)\n",
    "test_data = dataset.drop(training_data.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the native data structure in `PyTorch` is the `Tensor` and its default data types are long integers and single-precision floats, we now convert all objects into `Tensor` of type `torch.long` or `torch.float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into tensors\n",
    "training_labels = torch.tensor(training_data['class'].values, dtype=torch.long)\n",
    "training_data = torch.tensor(training_data.drop(columns='class').values, dtype=torch.float32)\n",
    "\n",
    "test_labels = torch.tensor(test_data['class'].values, dtype=torch.long)\n",
    "test_data = torch.tensor(test_data.drop(columns='class').values, dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, we can start designing a neural network for it. Identifying the optimal architecture for this problem is beyond the scope of this notebook. Our focus here is on understanding the building blocks of our neural network and its implementation. We could therefore try something like this:\n",
    "\n",
    "<img src=\"img/iris_network.svg\" style=\"background-color:white;padding:1em\">\n",
    "\n",
    "All neural networks (and, as a matter of fact, also all layers) are derived from the container `Module` found in `torch.nn`. We define the architecture in the subclass' `__init__` method. We also need to implement the `forward` method to define how our network is processing input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_length, hidden_layer_size, n_classes):\n",
    "        # Initialize the superclass first\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        \n",
    "        \"\"\" Now we can define the network's structure \"\"\"\n",
    "        # The network consists of a single, linear hidden layer...\n",
    "        self.hidden_layer = nn.Linear(input_length, hidden_layer_size)\n",
    "        # ...and an output layer\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Here we define the networks behavior as inputs are passed through it \"\"\"\n",
    "        # The output of the hidden layer is passed through a tanh activation function\n",
    "        hidden_layer_activation = torch.tanh(self.hidden_layer(x))\n",
    "        logits = self.output_layer(hidden_layer_activation)\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our network architecture, we can instantiate it by specifying the hidden layer size and the number of output classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = training_data.shape[1]\n",
    "n_species = len(training_labels.unique())\n",
    "net = MyNeuralNetwork(input_length=n_features, hidden_layer_size=8, n_classes=n_species)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is, our first neural network in PyTorch! 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, the network is not very useful, of course. We need to train it on our dataset so that it can learn patterns in the data. Training a neural network is a high-dimensional optimization problem that seeks to minimize the deviation of the predicted classes from the actual classes. This deviation is expressed by a loss function. Many loss functions have been developed and successfully applied in the field, and PyTorch offers a wide range of loss functions to choose from in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions). The choice of the loss function is a design decision that should be carefully weighed. In this example, we will go with the cross entropy loss, which has been shown to work well with multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Works well in this example, but there are many more options!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other important component in training is the choice of optimizer. The optimizer is essentially a strategy that determines how we update the weights and biases in our network based on a given loss. A number of optimization algorithms are implemented in the module [`torch.optim`](https://pytorch.org/docs/stable/optim.html#algorithms). Here, we will go with the popular stochastic gradient descent (SGD) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # Once again, there are many more options available!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we pass the parameters of our network to the optimizer here as a reference! This is how the network and the optimizer are connected: the optimizer can directly affect the networks parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process consists of an outer loop over the epochs (complete passes through the training set) and an inner loop over all observations (or batches of observations) in the training set. Inside the inner loop, we need to do five things:\n",
    "\n",
    "1. Reset the gradients of all the parameters\n",
    "2. Let the network genereate outputs \n",
    "3. Calculate the loss between the outputs and the true labels\n",
    "4. Compute the gradients of the loss for every parameter\n",
    "5. Let the optimizer update the parameters\n",
    "\n",
    "Rinse, repeat, until we have completed all iterations.\n",
    "\n",
    "Note that before we start training, it is good practice to set the model to \"training mode\". In our example here, there is no difference between training and evaluation time. But if we had, for example, included a dropout layer, then it is very important to flag the right mode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "net.train()\n",
    "\n",
    "print('Training', end='')\n",
    "for epoch in range(100):\n",
    "    if epoch % 10 == 0:\n",
    "        print('.', end='')\n",
    "        paired_data = list(zip(training_data, training_labels))\n",
    "        random.shuffle(paired_data)\n",
    "    for inputs, label in paired_data:\n",
    "        # Reset the optimizer so all gradients are equal to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = net(inputs)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, label)\n",
    "        # Computes the gradients\n",
    "        loss.backward()\n",
    "        # Optimize weights\n",
    "        optimizer.step()\n",
    "\n",
    "print('finished.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is trained! Now we can do a forward pass through the network and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()  # Don't forget to set the model to evaluation mode!\n",
    "predicted_logits = net.forward(test_data)\n",
    "predicted_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network does not give us a straight class label prediction. Instead, we get the output that we defined above in the forward method: The log-odds that the observation belongs to each of the three classes.\n",
    "\n",
    "We can obtain the class label by finding the most likely class, which corresponds to the index of the maximum log-odds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_class = predicted_logits.max(dim=1)\n",
    "predicted_class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare these predicted outputs to the actual class labels. Here, we will use some convenient reporting functions from `scikit-learn` (see workshop [*Intro to Machine Learning with scikit-learn*](https://git.dartmouth.edu/lib-digital-strategies/RDS/workshops/machine-learning/intro-to-machine-learning-with-scikit-learn)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(classification_report(test_labels, predicted_class, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👏 Nicely done! 💐🎉"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using a pre-trained \"off-the-shelf\" neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For our very simple *Iris* dataset, we could use a very simple network architecture. More complex problems, however, require more complex networks. More complex networks have more parameters, which requires more training samples and more training time. This complexity can quickly become overwhelming. \n",
    "\n",
    "Especially in computer vision and natural language processing, however, very powerful pre-trained models exist and are freely available to the general public. PyTorch makes it very easy to use such an \"off-the-shelf\" model! Not only can we quickly load the pretrained model, we can even get an entire preprocessing pipeline that makes sure our raw input is transformed into the format that the pre-trained network expects!\n",
    "\n",
    "For example, we can find a bunch of computer vision models in the module [`torchvision.models`](https://pytorch.org/vision/0.8/models.html). Another option is the [`torch.hub`](https://pytorch.org/docs/stable/hub.html#torch-hub), where you can even publish your own models!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate using the more complex [*Flowers* dataset ](https://doi.org/10.7910/DVN/1ECTVN). This dataset consists of labeled `*.jpg` images of five different kinds of flowers (see the [dataset's repository](https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/flowers-dataset)). Here, we will only use the daisies and roses to keep training times down, but feel free to experiment with the whole dataset!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "If you are running this notebook as part of a Reproducible Research workshop JupyterHub session, you can find the dataset in `/shared/RR-workshop-data/flowers-dataset`. If you are running this outside of Dartmouth's JupyterHub, you can download the [dataset here](https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/flowers-dataset/-/archive/main/flowers-dataset-main.zip). Unzip it to a location of your choosing and adjust the dataset path in the next cell accordingly!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Download the dataset as a zipped archive \"\"\" \n",
    "import urllib.request \n",
    "\n",
    "urllib.request.urlretrieve(\"https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/flowers-dataset/-/archive/main/flowers-dataset-main.zip\", \"dataset.zip\")\n",
    "\n",
    "\"\"\" Unzip the files \"\"\" \n",
    "import zipfile \n",
    "\t\n",
    "with zipfile.ZipFile(\"dataset.zip\", 'r') as zf:\n",
    "    zf.extractall()\n",
    "\n",
    "\"\"\" Clean up after ourselves \"\"\" \n",
    "import os \n",
    "os.remove(\"dataset.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_root = Path('./flowers-dataset-main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load a model architecture initialize it with the pretrained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT  # We even have different sets of pre-trained weights available! We'll go with the default here\n",
    "net = mobilenet_v2(weights=weights)\n",
    "net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that's a complex network! 😱 🤯"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we grab the preprocessing pipeline from the pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()\n",
    "preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the preprocessing consists of cropping and then resizing. We also get the mean and standard deviation from the training set used to train the weights that are used to standardize.\n",
    "\n",
    "\n",
    "Let's get some boilerplate code to read random image file and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "\n",
    "def get_random_image(test=False, classes=['roses', 'daisy']):\n",
    "    dataset = dataset_root\n",
    "    images = []\n",
    "    for class_ in classes:\n",
    "        subfolder = dataset / f\"{'test' if test else 'train'}\" / class_\n",
    "        images += [img for img in subfolder.glob('*.jpg')]\n",
    "    return read_image(str(random.choice(images)), ImageReadMode.RGB)\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some random images by running the next cell multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_random_image()\n",
    "imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that they vary in size and aspect ratio. Now let's look what happens when we preprocess them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess(img)\n",
    "imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, they are now resized and cropped to the same size and standardized (which means the colors look weird for the human eye)!\n",
    "\n",
    "Time to let the network do its thing and predict the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "img = get_random_image()\n",
    "imshow(img)\n",
    "img = preprocess(img)\n",
    "\n",
    "prediction = net(img.unsqueeze(0)).squeeze(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "\n",
    "print(category_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤨 Hmmm, that does not look right. 🤔\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Why do you think this highly sophisticated network does not give us the right answers?\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adapting an off-the-shelf neural network using transfer learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your problem happens to align with the problem that a network was trained to solve (e.g., your labels of interest are a subset of the labels used to train the model), you are in luck and can usually use a pre-trained network out-of-the-box. \n",
    "\n",
    "Often, however, your problem is slightly different. In this example, we are using photographic images just like `MobileNet`, but their labels are different than what `MobileNet` was originally trained with.\n",
    "\n",
    "Pre-trained networks can still be very valuable, however. You could think of a pre-trained network as a very sophisticated feature extractor:\n",
    "\n",
    "If we think of features as simply *numbers that describe an object*, then we could consider the output of any layer of the network, which are *numbers* that change depending on the network's input (the *object*), as features! From that point of view, the layers of a network are essentially a series of transforms of the input image to get to a feature representation. This feature vector is then classified by the final layer.\n",
    "\n",
    "So we can swap out that final classifier layer, use the very same features that `MobileNet` was trained to extract, and simply classify them with a classifier fitting our own problem (i.e., using the class labels we are interested in).\n",
    "\n",
    "We use most of the layers of `MobileNet` but insert our own final classification layer (or an entire other network!). We can then use the original layers as-is and only train the final layer. Since this means that we have way fewer parameters to train, we also need way fewer training samples.\n",
    "\n",
    "Let's walk through the process using our *Flowers* dataset, which is conveniently already split into a train and test subset.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Remember:** We will only use the images of daisies and roses in this tutorial to keep the training time down. You are welcome to try to classify all five flowers contained in the dataset, though!\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a list of all the image files we want to use for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_paths = []\n",
    "test_image_paths = []\n",
    "\n",
    "for class_ in ['daisy', 'roses']:\n",
    "    folder = dataset_root / 'train' / class_\n",
    "    training_image_paths += [file for file in folder.glob('*.jpg')]\n",
    "    folder = dataset_root / 'test' / class_\n",
    "    test_image_paths += [file for file in folder.glob('*.jpg')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will read and preprocess all the images and store them in a `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor holding all the training images...\n",
    "training_data = torch.stack([preprocess(read_image(str(file))) for file in training_image_paths])\n",
    "class_labels = ['daisy', 'roses']\n",
    "training_labels = torch.stack([torch.tensor(class_labels.index(str(file.parent.name))) for file in training_image_paths])\n",
    "\n",
    "# ...and one for the test images\n",
    "test_data = torch.stack([preprocess(read_image(str(file))) for file in test_image_paths])\n",
    "test_labels = torch.stack([torch.tensor(class_labels.index(str(file.parent.name))) for file in test_image_paths])\n",
    "\n",
    "print(f\"We have {len(training_data)} training images and {len(test_data)} test images.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Pro question:** What shape should the tensor `training_data` have now?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to make sure that all tensors are moved to the chosen device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.to(device)\n",
    "training_labels = training_labels.to(device)\n",
    "test_data = test_data.to(device)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to swap `MobileNet`'s classifier for an untrained one that fits our problem.\n",
    "\n",
    "First we make sure that none of `MobileNet`'s parameters get trained (since they are pre-trained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we swap in our simple, untrained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new classifier will take the same number of inputs as the old classifier\n",
    "n_features = net.classifier[1].in_features\n",
    "# Replace the original classifier with one that has the appropriate number of outputs \n",
    "net.classifier[1] = nn.Linear(n_features, len(class_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we move the network to the chosen device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is now basically the same as before. We need to define a loss and an optimizer and pass the parameters we want to optimize to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.classifier.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are only passing the classifier's parameters to the optimizer! Everything else would not get optimized, anyway (see above).\n",
    "\n",
    "Now we can actually do the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "# Don't forget to set the network to training mode!\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Iterate over the data\n",
    "    paired_data = list(zip(training_data, training_labels))\n",
    "    random.shuffle(paired_data)\n",
    "    for idx, (input, label) in enumerate(paired_data):\n",
    "        if idx % 100 == 0:\n",
    "            print('.', end='')\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate predictions\n",
    "        outputs = net(input.unsqueeze(0)).squeeze(0)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Calculate the gradients        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize the parameters\n",
    "        optimizer.step()\n",
    "    print(\"\")\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** We only did a single epoch here to keep the training time down. Feel free to do some additional epochs and see how it affects the networks performance!\n",
    "</div>\n",
    "\n",
    "Now that the network is trained, we can score its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "predictions = net(test_data)\n",
    "print(classification_report(y_pred=predictions.argmax(1).cpu(),  # Note: We need to make sure the tensors are in the main memory before passing them to code outside of PyTorch!\n",
    "                            y_true=test_labels.cpu(),            \n",
    "                            target_names=class_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for this relatively small amount of data! 🌹 🌼\n",
    "\n",
    "Let's look at some random examples again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_random_image(test=True)\n",
    "imshow(img)\n",
    "\n",
    "img = img.to(device)\n",
    "img = preprocess(img)\n",
    "\n",
    "prediction = net(img.unsqueeze(0)).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = class_labels[class_id]\n",
    "\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Far from perfect, but definitely a great start! With some additional tuning, we would have a pretty solid, complex classifier from very little data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next steps\n",
    "\n",
    "- [Official `PyTorch` tutorials](https://pytorch.org/tutorials/index.html)\n",
    "- The concept of [Datasets & Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "- Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
